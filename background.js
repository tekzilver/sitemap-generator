console.log("TEKZILVER: Background Script Loaded Successfully");

// --- State Management ---
let crawlQueue = [];
let visitedUrls = new Set();
let sitemapData = [];
let MAX_URLS = 500;
let isCrawling = false;
let robotsTxtRules = null;
let initialUrl = '';

// --- Robots.txt Parser ---
async function fetchRobotsTxt(baseUrl) {
  try {
    const robotsUrl = new URL('/robots.txt', baseUrl).href;
    const response = await fetch(robotsUrl);
    if (!response.ok) return null;
    const text = await response.text();
    return parseRobotsTxt(text);
  } catch (e) {
    console.warn('Could not fetch robots.txt', e);
    return null;
  }
}

function parseRobotsTxt(text) {
  const rules = { disallow: [] };
  let userAgentMatch = false;
  const lines = text.split('\n');
  for (let line of lines) {
    line = line.trim();
    if (line.toLowerCase().startsWith('user-agent:')) {
      const agent = line.split(':')[1].trim().toLowerCase();
      if (agent === '*') userAgentMatch = true;
      else userAgentMatch = false;
    } else if (userAgentMatch && line.toLowerCase().startsWith('disallow:')) {
      const path = line.split(':')[1].trim();
      if (path) rules.disallow.push(path);
    }
  }
  return rules;
}

function isAllowed(url, rules) {
  if (!rules) return true;
  const urlObj = new URL(url);
  const pathname = urlObj.pathname;
  for (let rule of rules.disallow) {
    if (pathname === rule || pathname.startsWith(rule)) return false;
  }
  return true;
}

// --- Main Crawler Logic ---
async function processQueue(originUrl) {
  if (visitedUrls.size >= MAX_URLS) {
    finishCrawl(true);
    return;
  }

  if (crawlQueue.length === 0) {
    finishCrawl(true);
    return;
  }

  let currentUrl = crawlQueue.shift();

  if (visitedUrls.has(currentUrl)) {
    processQueue(originUrl);
    return;
  }

  if (!isAllowed(currentUrl, robotsTxtRules)) {
    processQueue(originUrl);
    return;
  }

  try {
    visitedUrls.add(currentUrl);
    console.log(`TEKZILVER: Crawling ${currentUrl}... (${visitedUrls.size} total)`);
    
    chrome.runtime.sendMessage({
      action: 'PROGRESS_UPDATE',
      visited: visitedUrls.size,
      max: MAX_URLS
    });

    const response = await fetch(currentUrl, {
      method: 'GET',
      headers: {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
      }
    });
    
    const contentType = response.headers.get("content-type");
    if (!contentType || !contentType.includes("text/html")) {
      // Removed lastmod from sitemap data as requested
      sitemapData.push({ loc: currentUrl });
      processQueue(originUrl);
      return;
    }

    if (!response.ok) throw new Error(`HTTP ${response.status}`);

    const html = await response.text();
    
    // Removed lastmod from sitemap data as requested
    sitemapData.push({
      loc: currentUrl
    });

    // Regex to find links
    const linkRegex = /href\s*=\s*["'](.*?)["']/gi;
    let match;
    let foundLinks = new Set();

    while ((match = linkRegex.exec(html)) !== null) {
      let href = match[1];
      if (href.startsWith('javascript:') || href.startsWith('mailto:') || href.startsWith('#')) continue;

      try {
        let absoluteUrl = new URL(href, currentUrl).href;
        if (absoluteUrl.startsWith('http://')) {
            absoluteUrl = absoluteUrl.replace('http://', 'https://');
        }

        const baseDomain = new URL(originUrl).hostname;
        const linkDomain = new URL(absoluteUrl).hostname;

        if (baseDomain === linkDomain && 
            !visitedUrls.has(absoluteUrl) && 
            !foundLinks.has(absoluteUrl) &&
            visitedUrls.size + crawlQueue.length < MAX_URLS) {
          crawlQueue.push(absoluteUrl);
          foundLinks.add(absoluteUrl);
        }
      } catch (e) {}
    }

  } catch (error) {
    console.error(`TEKZILVER: Error on ${currentUrl}:`, error);
    if (currentUrl === initialUrl) {
      finishCrawl(false, `Failed to fetch start page: ${error.message}.`);
      return;
    }
  }

  setTimeout(() => processQueue(originUrl), 50); 
}

function finishCrawl(isSuccess, errorMessage = null) {
  isCrawling = false;

  if (!isSuccess) {
    chrome.runtime.sendMessage({ action: 'CRAWL_COMPLETE', success: false, message: errorMessage });
    return;
  }

  if (sitemapData.length === 0) {
    chrome.runtime.sendMessage({
      action: 'CRAWL_COMPLETE',
      success: false,
      message: "Found 0 URLs."
    });
    return;
  }

  // Added the "Generated by" comment at the top
  let xml = '<!-- Generated by Tekzilver.com -->\n';
  xml += '<?xml version="1.0" encoding="UTF-8"?>\n';
  xml += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n';
  
  sitemapData.forEach(item => {
    xml += '  <url>\n';
    xml += `    <loc>${escapeXml(item.loc)}</loc>\n`;
    // Removed lastmod line
    xml += '  </url>\n';
  });
  
  xml += '</urlset>';

  chrome.runtime.sendMessage({
    action: 'CRAWL_COMPLETE',
    success: true,
    xml: xml,
    count: sitemapData.length,
    baseUrl: initialUrl
  });
}

function escapeXml(unsafe) {
  return unsafe.replace(/[<>&'"]/g, function (c) {
    switch (c) {
      case '<': return '&lt;';
      case '>': return '&gt;';
      case '&': return '&amp;';
      case '\'': return '&apos;';
      case '"': return '&quot;';
    }
  });
}

// --- Message Listener ---
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  if (request.action === 'START_CRAWL') {
    console.log("TEKZILVER: Received START_CRAWL for:", request.url);
    
    if (isCrawling) {
      sendResponse({ success: false, message: 'Already crawling.' });
      return true;
    }

    const url = request.url;
    initialUrl = url; 
    
    crawlQueue = [url];
    visitedUrls.clear();
    sitemapData = [];
    isCrawling = true;

    fetchRobotsTxt(url).then(rules => {
      robotsTxtRules = rules;
      processQueue(url);
    });

    return true; 
  }
});